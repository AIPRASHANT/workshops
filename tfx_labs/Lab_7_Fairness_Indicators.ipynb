{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Fairness Indicators Example Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aalPefrUUplk"
      },
      "source": [
        "# [External Runtime] Fairness Indicators Example Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UZ48WFLwbCL6"
      },
      "source": [
        "**Overview**\n",
        "\n",
        "In this activity, you'll use Fairness Indicators to explore the Civil Comments dataset. Fairness Indicators is a suite of tools built on top of [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) that enable regular evaluation of fairness metrics in product pipelines.\n",
        "\n",
        "**About the Dataset**\n",
        "\n",
        "In this exercise, you'll work with the [Civil Comments dataset](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), approximately 2 million public comments made public by the [Civil Comments platform](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) in 2017 for ongoing research. This effort was sponsored by Jigsaw, who have hosted competitions on Kaggle to help classify toxic comments as well as minimize unintended model bias.\n",
        "\n",
        "Each individual text comment in the dataset has a toxicity label, with the label being 1 if the comment is toxic and 0 if the comment is non-toxic. Within the data, a subset of comments are labeled with a variety of identity attributes, including categories for gender, sexual orientation, religion, and race or ethnicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u33JXdluZ2lG"
      },
      "source": [
        "# Importing\n",
        "\n",
        "Run the following code to install the fairness_indicators library. This package contains the tools we'll be using in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoRNffG599XP",
        "colab": {}
      },
      "source": [
        "# TODO: !pip install fairness_indicators\n",
        "%tensorflow_version 1.x\n",
        "!pip install tensorflow_model_analysis\n",
        "!pip install tensorflow_data_validation\n",
        "!pip install --upgrade witwidget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mglfaM4_mtIk"
      },
      "source": [
        "**Restart the runtime as needed.** Run the following code to import the necessary dependencies for the libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8dlyTyiTe-9",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "import apache_beam as beam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_data_validation as tfdv\n",
        "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
        "from tensorflow_model_analysis.addons.fairness.view import widget_view\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "tf.get_logger().propagate = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYf5fGw4vlJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TsplOJGqWCf5"
      },
      "source": [
        "# Download the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vFOQ4AaIcAn2"
      },
      "source": [
        "In this exercise, you'll work with the Civil Comments dataset, approximately 2 million public comments made public by the Civil Comments platform in 2017.\n",
        "\n",
        "We've hosted the dataset on Google Cloud Platform for convenience. Run the following code to download the data from GCP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NdLBi6tN5i7I",
        "colab": {}
      },
      "source": [
        "train_tf_file = tf.keras.utils.get_file('train.tf', 'https://storage.googleapis.com/civil_comments_dataset/train.tfrecord')\n",
        "validate_tf_file = tf.keras.utils.get_file('validate.tf', 'https://storage.googleapis.com/civil_comments_dataset/validate.tfrecord')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ekzb7vVnPCc"
      },
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aioGdTyMdC9W"
      },
      "source": [
        "Here, we define the feature map that will be used to parse the data. Each example will have a label, comment text, and identity features `sexual orientation`, `gender`, `religion`, `race`, and `disability` that are associated with the text. See [this page](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) to learn more about the data schema. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n4_nXQDykX6W",
        "colab": {}
      },
      "source": [
        "BASE_DIR = tempfile.gettempdir()\n",
        "\n",
        "TEXT_FEATURE = 'comment_text'\n",
        "LABEL = 'toxicity'\n",
        "FEATURE_MAP = {\n",
        "    # Label:\n",
        "    LABEL: tf.io.FixedLenFeature([], tf.float32),\n",
        "    # Text:\n",
        "    TEXT_FEATURE:  tf.io.FixedLenFeature([], tf.string),\n",
        "\n",
        "    # Identities:\n",
        "    'sexual_orientation':tf.io.VarLenFeature(tf.string),\n",
        "    'gender':tf.io.VarLenFeature(tf.string),\n",
        "    'religion':tf.io.VarLenFeature(tf.string),\n",
        "    'race':tf.io.VarLenFeature(tf.string),\n",
        "    'disability':tf.io.VarLenFeature(tf.string),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mfbgerCsEOmN"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-naXyPdpdDmv"
      },
      "source": [
        "First, set up the input function to feed data into the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwoC-dzEDid3",
        "colab": {}
      },
      "source": [
        "def train_input_fn():\n",
        "  def parse_function(serialized):\n",
        "    parsed_example = tf.io.parse_single_example(\n",
        "        serialized=serialized, features=FEATURE_MAP)\n",
        "    # Adds a weight column to deal with unbalanced classes.\n",
        "    parsed_example['weight'] = tf.add(parsed_example[LABEL], 0.1)\n",
        "    return (parsed_example,\n",
        "            parsed_example[LABEL])\n",
        "  train_dataset = tf.data.TFRecordDataset(\n",
        "      filenames=[train_tf_file]).map(parse_function).batch(512)\n",
        "  return train_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nsJgv8zSdXhv"
      },
      "source": [
        "Next, create a deep neural network model, and train it on the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JaGvNrVijfws",
        "colab": {}
      },
      "source": [
        "model_dir = os.path.join(BASE_DIR, 'train', datetime.now().strftime(\n",
        "    \"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "embedded_text_feature_column = hub.text_embedding_column(\n",
        "    key=TEXT_FEATURE,\n",
        "    module_spec='https://tfhub.dev/google/nnlm-en-dim128/1')\n",
        "\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[500, 100],\n",
        "    weight_column='weight',\n",
        "    feature_columns=[embedded_text_feature_column],\n",
        "    n_classes=2,\n",
        "    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003),\n",
        "    model_dir=model_dir)\n",
        "\n",
        "classifier.train(input_fn=train_input_fn, steps=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTPqije9Eg5b"
      },
      "source": [
        "# Run TensorFlow Model Analysis with Fairness Indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7qgucgZmxCD"
      },
      "source": [
        "**TODO:** Description of TFMA and the added value of Fairness Indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-vRc-Jyp8dRm"
      },
      "source": [
        "## Export Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QLjiy5VCzlRw",
        "colab": {}
      },
      "source": [
        "def eval_input_receiver_fn():\n",
        "  serialized_tf_example = tf.compat.v1.placeholder(\n",
        "      dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
        "\n",
        "  # This *must* be a dictionary containing a single key 'examples', which\n",
        "  # points to the input placeholder.\n",
        "  receiver_tensors = {'examples': serialized_tf_example}\n",
        "\n",
        "  features = tf.io.parse_example(serialized_tf_example, FEATURE_MAP)\n",
        "  features['weight'] = tf.ones_like(features[LABEL])\n",
        "\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "    features=features,\n",
        "    receiver_tensors=receiver_tensors,\n",
        "    labels=features[LABEL])\n",
        "\n",
        "tfma_export_dir = tfma.export.export_eval_savedmodel(\n",
        "  estimator=classifier,\n",
        "  export_dir_base=os.path.join(BASE_DIR, 'tfma_eval_model'),\n",
        "  eval_input_receiver_fn=eval_input_receiver_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j8ODcee8rQ8"
      },
      "source": [
        "## Compute Fairness Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VKONqQyIfIs_"
      },
      "source": [
        "Render the Fairness Indicators widget with the exported evaluation results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7shDmJbx9mqa",
        "colab": {}
      },
      "source": [
        "tfma_eval_result_path = os.path.join(BASE_DIR, 'tfma_eval_result')\n",
        "\n",
        "# Define slices that you want the evaluation to run on.\n",
        "slice_spec = [\n",
        "    tfma.slicer.SingleSliceSpec(), # Overall slice\n",
        "    tfma.slicer.SingleSliceSpec(columns=['sexual_orientation']),\n",
        "    # You can uncomment the following lines for experiments on other identities.\n",
        "    # tfma.slicer.SingleSliceSpec(columns=['gender'],),\n",
        "    # tfma.slicer.SingleSliceSpec(columns=['religion']),\n",
        "    # tfma.slicer.SingleSliceSpec(columns=['race']),\n",
        "    # tfma.slicer.SingleSliceSpec(columns=['disability']),\n",
        "]\n",
        "\n",
        "# Add the fairness metrics.\n",
        "add_metrics_callbacks = [\n",
        "  tfma.post_export_metrics.fairness_indicators(\n",
        "      thresholds=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "      labels_key=LABEL\n",
        "      )\n",
        "]\n",
        "\n",
        "eval_shared_model = tfma.default_eval_shared_model(\n",
        "    eval_saved_model_path=tfma_export_dir,\n",
        "    add_metrics_callbacks=add_metrics_callbacks)\n",
        "\n",
        "validate_dataset = tf.data.TFRecordDataset(filenames=[validate_tf_file])\n",
        "\n",
        "# Run the fairness evaluation.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  _ = (\n",
        "      pipeline\n",
        "      | beam.Create([v.numpy() for v in validate_dataset])\n",
        "      | 'ExtractEvaluateAndWriteResults' >>\n",
        "       tfma.ExtractEvaluateAndWriteResults(\n",
        "                 eval_shared_model=eval_shared_model,\n",
        "                 slice_spec=slice_spec,\n",
        "                 compute_confidence_intervals=True,\n",
        "                 output_path=tfma_eval_result_path)\n",
        "  )\n",
        "\n",
        "eval_result = tfma.load_eval_result(output_path=tfma_eval_result_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNaNhTCTAMHm",
        "colab": {}
      },
      "source": [
        "widget_view.render_fairness_indicator(eval_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jtDpTBPeRw2d"
      },
      "source": [
        "# Run What-if Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKgyEd8nfR2b"
      },
      "source": [
        "In this section, you'll use the [What-If Tool's ](https://pair-code.github.io/what-if-tool/)interactive visual interface to explore and manipulate data at a micro-level.\n",
        "\n",
        "You can use the **Binning**, **Color By**, **Label by**, and **Scatter** dropdowns at the top of the What-If widget to create a visualization that groups examples by sexual orientation, and displays both how each example was categorized (`inference_label`) by the model and its actual ground-truth label (`toxicity`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wtjZo4BDlV1m",
        "colab": {}
      },
      "source": [
        "num_datapoints = 1000\n",
        "def wit_dataset(file, num_datapoints):\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      filenames=[train_tf_file]).take(num_datapoints)\n",
        "  return [tf.train.Example.FromString(d.numpy()) for d in dataset]\n",
        "\n",
        "data = wit_dataset(train_tf_file, num_datapoints)\n",
        "config_builder = WitConfigBuilder(data).set_estimator_and_feature_spec(\n",
        "    classifier, FEATURE_MAP).set_label_vocab(['non-toxicity', LABEL]).set_target_feature(LABEL)\n",
        "wit = WitWidget(config_builder)\n",
        "wit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "osyG2Si0fSgw"
      },
      "source": [
        "In the Fairness Indicators widget above, you can click on a particular a slice to further explore it in the What-If Tool. The data in that slice will appear in the What-If Tool widget for further inspection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QazvgInxglaP"
      },
      "source": [
        "# Run TensorFlow Data Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ho7OG7T4gnwB"
      },
      "source": [
        "[TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) is another tool you can use to analyze your data. You can use it to find potential problems in your data, such as missing values and data imbalances, that can lead to Fairness disparities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7dORXr7lWqh",
        "colab": {}
      },
      "source": [
        "stats = tfdv.generate_statistics_from_tfrecord(data_location=train_tf_file)\n",
        "tfdv.visualize_statistics(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-FVjP7sxWWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}